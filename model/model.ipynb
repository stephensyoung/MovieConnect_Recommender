{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "ratings = pd.read_csv('ml-latest-small/tags.csv')  # Contains userId, movieId, rating, timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data: Map userId and movieId to a continuous range of indexes\n",
    "user_ids = ratings['userId'].unique()\n",
    "movie_ids = ratings['movieId'].unique()\n",
    "user_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "movie_to_index = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "ratings['userId'] = ratings['userId'].apply(lambda x: user_to_index[x])\n",
    "ratings['movieId'] = ratings['movieId'].apply(lambda x: movie_to_index[x])\n",
    "\n",
    "# Data Splitting\n",
    "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'rating'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rating'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     11\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_ids[idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmovie_ids[idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mratings[idx]\n\u001b[1;32m---> 13\u001b[0m train_dataset \u001b[39m=\u001b[39m MovieLensDataset(train_df[\u001b[39m'\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, train_df[\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, train_df[\u001b[39m'\u001b[39;49m\u001b[39mrating\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m     14\u001b[0m test_dataset \u001b[39m=\u001b[39m MovieLensDataset(test_df[\u001b[39m'\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, test_df[\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, test_df[\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m     16\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:3760\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3759\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3760\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3761\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3762\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rating'"
     ]
    }
   ],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, user_ids, movie_ids, ratings):\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "        self.movie_ids = torch.tensor(movie_ids, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(ratings, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.movie_ids[idx], self.ratings[idx]\n",
    "\n",
    "train_dataset = MovieLensDataset(train_df['userId'].values, train_df['movieId'].values, train_df['rating'].values)\n",
    "test_dataset = MovieLensDataset(test_df['userId'].values, test_df['movieId'].values, test_df['rating'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_size):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_size)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        user_embedded = self.user_embedding(user_ids)\n",
    "        movie_embedded = self.movie_embedding(movie_ids)\n",
    "        x = torch.cat([user_embedded, movie_embedded], dim=1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "model = RecommenderSystem(num_users, num_movies, embedding_size=50)\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.891633173696634\n"
     ]
    }
   ],
   "source": [
    "def train(model, loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for users, movies, ratings in loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(loader)}')\n",
    "\n",
    "train(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load data\n",
    "ratings = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "# Assume movies.csv is also loaded if needed for metadata\n",
    "# movies = pd.read_csv('movies.csv')\n",
    "\n",
    "# Map user IDs and movie IDs to a continuous range of integers\n",
    "user_ids = ratings['userId'].unique()\n",
    "movie_ids = ratings['movieId'].unique()\n",
    "user_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "movie_to_index = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "ratings['userId'] = ratings['userId'].apply(lambda x: user_to_index[x])\n",
    "ratings['movieId'] = ratings['movieId'].apply(lambda x: movie_to_index[x])\n",
    "\n",
    "# Split the dataset\n",
    "train_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, user_ids, movie_ids, ratings):\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "        self.movie_ids = torch.tensor(movie_ids, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(ratings, dtype=torch.float)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.user_ids[idx], self.movie_ids[idx], self.ratings[idx]\n",
    "\n",
    "train_dataset = MovieLensDataset(train_df['userId'].values, train_df['movieId'].values, train_df['rating'].values)\n",
    "test_dataset = MovieLensDataset(test_df['userId'].values, test_df['movieId'].values, test_df['rating'].values)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderSystem(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_size):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_size)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_size * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, user_ids, movie_ids):\n",
    "        user_embedded = self.user_embedding(user_ids)\n",
    "        movie_embedded = self.movie_embedding(movie_ids)\n",
    "        x = torch.cat([user_embedded, movie_embedded], dim=1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user_ids)\n",
    "num_movies = len(movie_ids)\n",
    "model = RecommenderSystem(num_users, num_movies, embedding_size=50)\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.159167663206028\n",
      "Epoch 2, Loss: 1.3757724354538736\n",
      "Epoch 3, Loss: 1.2632743082469022\n",
      "Epoch 4, Loss: 1.1864377301705034\n",
      "Epoch 5, Loss: 1.1400051132033142\n"
     ]
    }
   ],
   "source": [
    "def train(model, loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for users, movies, ratings in loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(users, movies)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(loader)}')\n",
    "\n",
    "train(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'ML_Small_State_Dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RecommenderSystem()\n",
    "model2.load_state_dict(torch.load(\"ML_Small_State_Dict.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/serving-pytorch-models-with-torchserve-6b8e8cbdb632"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
